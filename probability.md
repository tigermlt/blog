- Events: A thing that can happen

- Experiments: Have events as outcomes

- Inclusion-Exclusion Principle:
  - If outcomes can come from set A or B, then the total number of outcomes is:
    <img src="https://latex.codecogs.com/svg.latex?|A&space;\cup&space;B|&space;=&space;|A|&space;&plus;&space;|B|&space;-&space;|A&space;\cap&space;B|" title="|A \cup B| = |A| + |B| - |A \cap B|" />

- Product rule of counting:
  - If outcomes are generated via a process with r steps, where step i has ni outcomes, then the total number of outcomes is: n1 * n2 * ... * nr
  
- combinatorics:
  - sort objects (permutation)
    - distinct: if n is the total number of elements to sort, then the total number of ways to sort is n!
    - semi-distinct: if there are n objects, n1 are the same (within the group), n2 are the same (with in the group) ...
    then there are <img src="https://latex.codecogs.com/svg.latex?\frac{n!}{n_{1}!...n_{r}!}" title="\frac{n!}{n_{1}!...n_{r}!}" /> unique orderings (permutations)
  - choose k objects (combinations) from distinct items
    - for instance, there are n = 20 people, choose k = 5 people to get cake and the ordering of those 5 does not matter
    - <img src="https://latex.codecogs.com/svg.latex?\binom{n}{k}&space;=&space;\frac{n!}{k!(n-k)!}" title="\binom{n}{k} = \frac{n!}{k!(n-k)!}" />
  - put objects into r buckets
    - items are distinct: r^n where n is number of items
    - items are not distinct: <img src="https://latex.codecogs.com/svg.latex?\frac{(n&plus;r-1)!}{n!(r-1)!}" title="\frac{(n+r-1)!}{n!(r-1)!}" /> 
- Axioms of probability:
  - 0 <= P(E) <= 1
  - P(S) = 1
  - P(E^c) = 1 - P(E) or P(E union F) = p(E) + p(F) if event E and F are mutually exclusive
- conditional probability: it is the probability that E occurs given that F has already occurred
  - written as ```P(E|F)``` : means P(E, given F already observed)
  - the definition of conditional probability: <img src="https://latex.codecogs.com/svg.latex?P(E|F)&space;=&space;\frac{P(E\cap&space;F)}{P(F)}" title="P(E|F) = \frac{P(E\cap F)}{P(F)}" />, this holds even when outcomes are not equally likely
  - In general: <img src="https://latex.codecogs.com/svg.latex?P(E1E2...En)&space;=&space;P(E1)P(E2|E1)...P(En|E1E2...En-1)" title="P(E1E2...En) = P(E1)P(E2|E1)...P(En|E1E2...En-1)" />
  - ![paradigm](https://tigermlt.github.io/blog/conditional_paradigm.png)
  - machine learning is: probability + data + computers
- Law of total probability:
  - <img src="https://latex.codecogs.com/svg.latex?P(E)&space;=&space;\sum&space;_{i}P(B_{i}\cap&space;E)&space;=&space;\sum&space;_{i}P(E&space;|B_{i})&space;\times&space;P(B_{i})" title="P(E) = \sum _{i}P(B_{i}\cap E) = \sum _{i}P(E |B_{i}) \times P(B_{i})" />
  - Bi are mutually exclusive
- Bayes Theorem
  - <img src="https://latex.codecogs.com/gif.latex?P(E|F)&space;=&space;\frac{P(F|E)P(E)}{P(F)}" title="P(E|F) = \frac{P(F|E)P(E)}{P(F)}" />
  - <img src="https://latex.codecogs.com/gif.latex?P(E|F)&space;=&space;\frac{P(F|E)P(E)}{P(F|E)P(E)&space;&plus;&space;P(F|E^{c})P(E^{c})}" title="P(E|F) = \frac{P(F|E)P(E)}{P(F|E)P(E) + P(F|E^{c})P(E^{c})}" /> it is also equal to the above equation after applying low of total probability
  - update belief
    - we have some P(location) before observation, know ```P(observation|location)```, update P(location)
    - after observation, we want to compute ```P(L|O)```, use bayes theorem and law of total probability to compute it
- independence
  - if two events E and F are mutually exclusive, then P(E union F) = P(E) + P(F)
  - if two events E and F are not mutually exclusive, then P(E union F) = P(E) + P(F) - P(EF)
  - for three sets, P(E union F union G) = P(E) + P(F) + P(G) - p(EF) - p(EG) - P(FG) + P(EFG). So if there is no mutually exclusion, life will be hard
  - two events A and B are independent if P(AB) = P(A)P(B) otherwise they are dependent events. Another definition for indepedent is ```P(A|B)``` = P(A)
  - if events A and B are independent, prove that A and B compliment are independent
    <img src="https://latex.codecogs.com/gif.latex?P(AB^{c})&space;=&space;P(A)&space;-&space;P(AB)&space;=&space;P(A)&space;-&space;P(A)P(B)&space;=&space;P(A)[1-P(B)]&space;=&space;P(A)P(B^{c})" title="P(AB^{c}) = P(A) - P(AB) = P(A) - P(A)P(B) = P(A)[1-P(B)] = P(A)P(B^{c})" />
    - the first equality comes from lawy of total probability, P(A) = P(ABc) + P(AB)
  - general definition of independence is that for verey subset with r elements, P(E1...Er) = P(E1)...P(Er)
  - example, p is the probability to get head by rolling a dice, what is P(exactly k heads on n coin flips)
    - each coin flip are independent and the probablity of getting one case of k head is: <img src="https://latex.codecogs.com/gif.latex?P(F_{i})&space;=&space;p^{k}(1-p)^{n-k}" title="P(F_{i}) = p^{k}(1-p)^{n-k}" />
    - so the total probablity is: <img src="https://latex.codecogs.com/gif.latex?\binom{n}{k}p^{k}(1-p)^{n-k}" title="\binom{n}{k}p^{k}(1-p)^{n-k}" />
- set operations
  - DeMorgan's laws: <img src="https://latex.codecogs.com/gif.latex?(E\cup&space;F)^{c}&space;=&space;E^{c}&space;\cap&space;F^{c}" title="(E\cup F)^{c} = E^{c} \cap F^{c}" />; <img src="https://latex.codecogs.com/gif.latex?(E\cap&space;F)^{c}&space;=&space;E^{c}&space;\cup&space;F^{c}" title="(E\cap F)^{c} = E^{c} \cup F^{c}" />
- conditional independence:
  - in the conditional paradigm, the formulas of probability are preserved
  - <img src="https://latex.codecogs.com/gif.latex?P(A|BE)&space;=&space;\frac{P(B|AE)P(A|E)}{P(B|E)}" title="P(A|BE) = \frac{P(B|AE)P(A|E)}{P(B|E)}" />, similar to the previous equation, just add condition on E for every term
  - independence relationships can change with conditioning (if events E and F are independent, that does not mean they will still be independent given another event G)
  - two events E and F are conditionally independent given G if ```P(EF|G) = P(E|G)P(F|G) or P(E|FG) = P(E|G)```
- Random variables: it is a variable will have a value but there is uncertainty as to what value
  - for instance, Y = number of "heads" on 3 coins. Y is a random variable, Y could be 0, 1, 2, 3
  - with random variable, we can do:
    - probability mass function P(X=a) which means the probability of a random variable take particular value. Can think of it as a function, given a value, output a probability. Notice the random variable is **discrete**
      - the notation could also be written as p(x) or <img src="https://latex.codecogs.com/gif.latex?P_{X}(x)" title="P_{X}(x)" />
    - expectation E[X]
      - for discrete random variable X: <img src="https://latex.codecogs.com/gif.latex?E[X]&space;=&space;\sum&space;_{x:p(x)>0}x*p(x)" title="E[X] = \sum _{x:p(x)>0}x*p(x)" />, in other words, sum over all values of x that have PMF bigger than 0
      - other semantic meaning: mean, weighted average, center of mass, 1st moment
      - properties:
        - linearity: E[aX+b] = aE[x] + b
        - expectation of a sum is the sum of expectations: E[X + Y] = E[X] = E[Y]
        - unconscious statistician: <img src="https://latex.codecogs.com/gif.latex?E[g(x)]&space;=&space;\sum&space;_{x}&space;g(x)p(x)" title="E[g(x)] = \sum _{x} g(x)p(x)" />
    - variance: Var(X)
      - think about value x and the difference to the mean E[x]. How to get the average difference, one way is to do average of the absolute value of difference (L1 norm); the other way is to average over the square of the difference (L2 norm)
      - we use L2 norm in variance and variance is a formal representation of "spread"
      - <img src="https://latex.codecogs.com/gif.latex?Var(x)&space;=&space;E[(x-\mu&space;)^{2}]" title="Var(x) = E[(x-\mu )^{2}]" /> for X being a random variable = <img src="https://latex.codecogs.com/gif.latex?\sum&space;_{x}&space;(x-\mu&space;)^{2}p(x)" title="\sum _{x} (x-\mu )^{2}p(x)" /> = <img src="https://latex.codecogs.com/gif.latex?E[x^2]&space;-&space;(E[x])^2" title="E[x^2] - (E[x])^2" />
      - standard deviation is the square root of variance
      - also called 2nd central moment
      - properties of variance:
        - var(aX+b) = a^2Var(x)
  - Bernoulli Random Variable
    - experiment results in either "success" or "failure"
    - P(X=1) = p, P(X=0) = 1-p; X is a Bernoulli Random variable denoted as X ~ Ber(p)
    - E[X] = p, Var(X) = p(1-p)
    - some examples in the real world: coin flip, random binary digit
  - Binomial Random Variable
    - consider n independent trials of Ber(p) random variable, X is number of successes in n trials
    - In this case, X is a Binomial Random Variable: X~Bin(n,p)
    - P(X=i) = p(i) = <img src="https://latex.codecogs.com/gif.latex?\binom{n}{i}p^{i}(1-p)^{n-i}" title="\binom{n}{i}p^{i}(1-p)^{n-i}" />, i=0,1...n
    - the summation of P(X=i) for all i is 1
    - some examples in the real world: # of heads in n coin flips, # of 1s in randomly generated length n bit string
    - E[X] = np, Var(X) = np(1-p)
    - Ber(p) = Bin(1,p)
    - Galton Board, if we want to represent whether a particular marble goes right, it is a Bernoulli random variable R~Ber(0.5); if we look at what bucket a marble lands in, then it is binominal random variable B~Bin(levels, 0.5)
- Poisson distribution:
  - A formulat that is about natural experssion: <img src="https://latex.codecogs.com/gif.latex?\lim_{n&space;\to&space;\infty}&space;(1-\frac{\lambda}{n})^{n}&space;=&space;e^{-\lambda}" title="\lim_{n \to \infty} (1-\frac{\lambda}{n})^{n} = e^{-\lambda}" />
  - one usage of it is to determine the probability of haveing n requests if given lambda = k requests among a region in one minute
    - we can model this as a binominal distribution. If the granularity is second, then it can be expressed as X~Bin(60, k/60)
    - if the granularity is milisecond, it can be expressed as X~Bin(60000, k/60000)
    - if the granularity is so small, we have X~Bin(infinity, k/infinity)
  - Binomial in the limit:
    - <img src="https://latex.codecogs.com/gif.latex?P(X=k)&space;=&space;\lim_{x&space;\to&space;\infty}&space;\binom{n}{k}(\frac{\lambda}{k})^{k}(1-\frac{\lambda}{k})^{n-k}&space;=&space;\lim_{x&space;\to&space;\infty}&space;\frac{n!}{(n-k)!k!}*\frac{(\lambda)^k}{n^{k}}*\frac{(1-\lambda/n)^{n}}{(1-\lambda/n)^{k}}&space;=&space;\lim_{x&space;\to&space;\infty}&space;\frac{n!}{(n-k)!k!}*\frac{(\lambda)^k}{n^{k}}*\frac{e^{-\lambda}}{1}&space;=&space;\lim_{x&space;\to&space;\infty}&space;\frac{n!}{(n-k)!n^{k}}*\frac{(\lambda)^k}{k!}*\frac{e^{-\lambda}}{1}&space;=&space;\lim_{x&space;\to&space;\infty}&space;\frac{n^{k}}{n^{k}}*\frac{(\lambda)^k}{k!}*\frac{e^{-\lambda}}{1}&space;=&space;\frac{\lambda^{k}e^{-\lambda}}{k!}" title="P(X=k) = \lim_{x \to \infty} \binom{n}{k}(\frac{\lambda}{k})^{k}(1-\frac{\lambda}{k})^{n-k} = \lim_{x \to \infty} \frac{n!}{(n-k)!k!}*\frac{(\lambda)^k}{n^{k}}*\frac{(1-\lambda/n)^{n}}{(1-\lambda/n)^{k}} = \lim_{x \to \infty} \frac{n!}{(n-k)!k!}*\frac{(\lambda)^k}{n^{k}}*\frac{e^{-\lambda}}{1} = \lim_{x \to \infty} \frac{n!}{(n-k)!n^{k}}*\frac{(\lambda)^k}{k!}*\frac{e^{-\lambda}}{1} = \lim_{x \to \infty} \frac{n^{k}}{n^{k}}*\frac{(\lambda)^k}{k!}*\frac{e^{-\lambda}}{1} = \frac{\lambda^{k}e^{-\lambda}}{k!}" />
    - equation 2 to 3 uses natural expression
    - equation 4 to 5 uses limit analysis, since n is large n!/(n-k)! = n^k
  - Poisson Random variable: the number of occurrences in a fixed interval of time
    - X ~ Poi(lambda)
    - lambda is the "rate"
    - X takes on values 0,1,2..
    - it has PMF P(X=k) = <img src="https://latex.codecogs.com/gif.latex?\frac{\lambda^{k}e^{-\lambda}}{k!}" title="\frac{\lambda^{k}e^{-\lambda}}{k!}" />
    - usage: earthquakes, radioactive decay, hits to web server; have time interval for events
    - discrete
    - events are independent
    - Poisson is greate when you have a "rate" and make sure the rate has the correct unit that is asking
    - Poisson can be used to approximate Binominal when n is large, p is small. lambda = n*p (we expect np to be moderate large, such as n>20, p<0.05)
    - X ~ Poi(lambda) where lambda = np (n->infinity, p -> 0)
      - E[X] = np = lambda (recall the expectation of binomial distribution)
      - Var(x) = np(1-p) = lambda * (1-0) = lambda
    - Can still apply Poisson approximation when independent assumption (or same probability p) is "mildly" violated
      - number of entries in each bucket in large hash table
      - average number of requests to web server
      - Notice that the PMF can be adapted if, instead of the average number of events \lambda , we are given a time rate r for the events to happen. Then lambda =rt with r in units of 1/time, and <a href="https://www.codecogs.com/eqnedit.php?latex=P(k&space;\hspace{1mm}&space;events&space;\hspace{1mm}&space;in&space;\hspace{1mm}&space;interval\hspace{1mm}&space;t)&space;=&space;e^{-rt}\frac{(rt)^k}{k!}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?P(k&space;\hspace{1mm}&space;events&space;\hspace{1mm}&space;in&space;\hspace{1mm}&space;interval\hspace{1mm}&space;t)&space;=&space;e^{-rt}\frac{(rt)^k}{k!}" title="P(k \hspace{1mm} events \hspace{1mm} in \hspace{1mm} interval\hspace{1mm} t) = e^{-rt}\frac{(rt)^k}{k!}" /></a>

- A midpoint summary
  -
      |                  | number of successes | number of time to get success |
      |------------------|---------------------|-------------------------------|
      | one trial        | X~Ber(p)            | X~Geo(p)                      |
      | several trials   | X~Bin(n,p)          | X~NegBin(r,p)                 |
      | interval of time | X~Poi(lambda)       | X~Zipf                        |
- Geometric Random Variable
  - X is number of independent trials until first success
  - p is probability of success on each trial
  - <a href="https://www.codecogs.com/eqnedit.php?latex=P(X=n)&space;=&space;(1-p)^{n-1}p" target="_blank"><img src="https://latex.codecogs.com/gif.latex?P(X=n)&space;=&space;(1-p)^{n-1}p" title="P(X=n) = (1-p)^{n-1}p" /></a>
  - E[X] = 1/p
  - Var(X) = (1-p)/p^2
- Negative Binomial Random Variable
  - X ~ NegBin(r,p)
  - X is number of independent trials until r successes
  - p is probability of success on each trial
  - P(X=n) = <a href="https://www.codecogs.com/eqnedit.php?latex=P(X=n)&space;=&space;\binom{n-1}{r-1}p^{r}(1-p)^{n-r}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?P(X=n)&space;=&space;\binom{n-1}{r-1}p^{r}(1-p)^{n-r}" title="P(X=n) = \binom{n-1}{r-1}p^{r}(1-p)^{n-r}" /></a> where n = r, r+1...
  - E[X] = r/p
  - Var(X) = r(1-p)/p^2
  - Geo(p) ~ NegBin(1,p)
- Bit Coin Mining:
  - You "mine a bitcoin" if for given data D, you find a number N such that Hash(D, N) produces a string that starts with g zeros.  Solve using geometric random variable equation.
- Continuous Random Variable
  - The **probability density function (PDF)** of a continous random variable represents the relative likelihood of various values. <a href="https://www.codecogs.com/eqnedit.php?latex=P(a<X<b)&space;=&space;\int_{x=a}^{b}f(X=x)dx" target="_blank"><img src="https://latex.codecogs.com/gif.latex?P(a<X<b)&space;=&space;\int_{x=a}^{b}f(X=x)dx" title="P(a<X<b) = \int_{x=a}^{b}f(X=x)dx" /></a> or <a href="https://www.codecogs.com/eqnedit.php?latex=P(a<X<b)&space;=&space;\int_{x=a}^{b}f_{X}(x)dx" target="_blank"><img src="https://latex.codecogs.com/gif.latex?P(a<X<b)&space;=&space;\int_{x=a}^{b}f_{X}(x)dx" title="P(a<X<b) = \int_{x=a}^{b}f_{X}(x)dx" /></a> in a different notation
  - Properties of PDF:
    - the value is >=0, <=1
    - integrate from -infinity to +infinity is 1
    - PDF articulate relative belief, the integration of it is a probability
    - f(X=x) is not a probability, it could be greater than 1
  - Expectation: <a href="https://www.codecogs.com/eqnedit.php?latex=E[X]&space;=&space;\int_{-&space;\infty}^{\infty}xf_{X}(x)dx" target="_blank"><img src="https://latex.codecogs.com/gif.latex?E[X]&space;=&space;\int_{-&space;\infty}^{\infty}xf_{X}(x)dx" title="E[X] = \int_{- \infty}^{\infty}xf_{X}(x)dx" /></a>
    - The properties of expectation of discrete RV still hold
  - Variance: <a href="https://www.codecogs.com/eqnedit.php?latex=Var(X)&space;=&space;\int_{-&space;\infty}^{\infty}(x-\mu&space;)^{2}p(x)dx&space;=&space;E[X^2]&space;-&space;(E[X])^{2}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?Var(X)&space;=&space;\int_{-&space;\infty}^{\infty}(x-\mu&space;)^{2}p(x)dx&space;=&space;E[X^2]&space;-&space;(E[X])^{2}" title="Var(X) = \int_{- \infty}^{\infty}(x-\mu )^{2}p(x)dx = E[X^2] - (E[X])^{2}" /></a>
  - Uniform Random Variable
    - X ~ <a href="https://www.codecogs.com/eqnedit.php?latex=Uni(\alpha&space;,&space;\beta&space;)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?Uni(\alpha&space;,&space;\beta&space;)" title="Uni(\alpha , \beta )" /></a>
    - PDF: <a href="https://www.codecogs.com/eqnedit.php?latex=f(X=x)&space;=&space;\frac{1}{\beta&space;-&space;\alpha&space;}&space;if&space;\alpha&space;<=x&space;<=&space;\beta" target="_blank"><img src="https://latex.codecogs.com/gif.latex?f(X=x)&space;=&space;\frac{1}{\beta&space;-&space;\alpha&space;}&space;if&space;\alpha&space;<=x&space;<=&space;\beta" title="f(X=x) = \frac{1}{\beta - \alpha } if \alpha <=x <= \beta" /></a>  otherwise f(X=x) is 0
      - <a href="https://www.codecogs.com/eqnedit.php?latex=E[X]&space;=&space;\frac{\beta&space;-&space;\alpha}{2}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?E[X]&space;=&space;\frac{\beta&space;-&space;\alpha}{2}" title="E[X] = \frac{\beta - \alpha}{2}" /></a>
      - <a href="https://www.codecogs.com/eqnedit.php?latex=Var(x)&space;=&space;\frac{(\beta&space;-&space;\alpha)^2}{12}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?Var(x)&space;=&space;\frac{(\beta&space;-&space;\alpha)^2}{12}" title="Var(x) = \frac{(\beta - \alpha)^2}{12}" /></a>
  - Exponential Random Variable
    - continuous equivalent of Poisson distribution, it represents time we need to wait until some event (such as eqrthquake, request to web server, end cell phone contract etc) given constant rate
    - X ~ <a href="https://www.codecogs.com/eqnedit.php?latex=Exp(\lambda)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?Exp(\lambda)" title="Exp(\lambda)" /></a>, rate \lambda > 0
    - PDF: <a href="https://www.codecogs.com/eqnedit.php?latex=f(X=x)&space;=&space;\lambda&space;e^{-\lambda&space;x}&space;x&space;>=0" target="_blank"><img src="https://latex.codecogs.com/gif.latex?f(X=x)&space;=&space;\lambda&space;e^{-\lambda&space;x}&space;x&space;>=0" title="f(X=x) = \lambda e^{-\lambda x} x >=0" /></a> otherwise f(X=x) = 0
    - E[X] = 1/lambda
    - Var(X) = 1/(lambda)^2 
    - support: X>=0
    - for instance if given the rate of earthquick per year, ask what is the probability of having zero major earthquake next year, use poisson; if ask what is the probability of a marjor earthquake in the next 30 years, use exponential RV
  - Cumulative Density Function: it is a "closed form" equation for the probability that a random variable is less than a given value F(x) = P(X<x). It can be used to avoid integrals
    - Short hand notation: <a href="https://www.codecogs.com/eqnedit.php?latex=F_{X}x" target="_blank"><img src="https://latex.codecogs.com/gif.latex?F_{X}x" title="F_{X}x" /></a>
    - the CDF of exponential RV is: <a href="https://www.codecogs.com/eqnedit.php?latex=F_{X}x&space;=&space;1&space;-&space;e^{-\lambda&space;x}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?F_{X}x&space;=&space;1&space;-&space;e^{-\lambda&space;x}" title="F_{X}x = 1 - e^{-\lambda x}" /></a>
  - A quick notation summary:
    - p(a) or p_{X}(a): probability mass function (discrete) P(X=a)
    - f(a) or f_{X}(a): probability density function (continuous) f(X=a)
    - F(a) or F_{X}(a): cumulative distribution function P(X<=a)
  - Normal distribution
    - X ~ <a href="https://www.codecogs.com/eqnedit.php?latex=N(\mu&space;,&space;\sigma&space;^2)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?N(\mu&space;,&space;\sigma&space;^2)" title="N(\mu , \sigma ^2)" /></a>
    - PDF: f(x) = <a href="https://www.codecogs.com/eqnedit.php?latex=\frac{1}{\sigma&space;\sqrt{2&space;\pi}}e^{-(x-\mu)^2/2&space;\sigma&space;^2}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\frac{1}{\sigma&space;\sqrt{2&space;\pi}}e^{-(x-\mu)^2/2&space;\sigma&space;^2}" title="\frac{1}{\sigma \sqrt{2 \pi}}e^{-(x-\mu)^2/2 \sigma ^2}" /></a> where -\infinity < x < \inifinity
    - E[X] = \mu
    - Var(X) = \signma ^2
    - Also called Gaussian, f(x) is symmetric about \mu
    - why use normal distribution:
      - common for natural phenomena (such as heights, weights) (it is log-normal in reality)
      - often results from the equally weighted sum of multiple variables
      - most noise is normal (at least assumed to be normal)
      - means of samples are distributed normally
      - more importantly, it is the least assuming distribution (simple and will generalize). It maximizes entropy (measures mathematical disorder) for a given mean and variance
     - No colosed form for CDF
     - Look up table method: F(x) = <a href="https://www.codecogs.com/eqnedit.php?latex=\Phi&space;(\frac{x&space;-&space;\mu}{\sigma})" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\Phi&space;(\frac{x&space;-&space;\mu}{\sigma})" title="\Phi (\frac{x - \mu}{\sigma})" /></a> where \phi is a function that has been solved for numerically for \mu = 0, \sigma = 1 (standard normal)
     - Linear Transformation of normal is normal. Y = aX + b is also normal if X is normal. E[Y] = aE[X] + b; Var[Y] = a^2 \sigma ^2. So Y ~ N(a \mu + b, a^2 \sigma ^ 2)
        - a special case of linear transform: Z = (X - \mu)/(\sigma) ~ N(0,1). This is called the standard normal distribution. 
          - \phi (-a) = 1 - \phi(a)
          - P(c < Z < d) = \phi (d) - \phi (c)
     - So for X ~ N(\mu, \sigma ^2), it is equal to \phi((x-\mu)/ \sigma)
- Binomial approximation and joint distributions
  - relative probability: ratio of probability. For instance P(X=10)/P(X=5) = delta*f(X=10)/delta*f(X=5) = f(X=10)/f(X=5) and use the PDF formula to get the ratio. The delta technique is used here to simulate integration
  - Normal approximations binomial. For instance Bin(100, 0.5) ~ Normal(50,25) since E[X] = np, Var(X) = np(1-p)
    - one nuance about the approximation is about continuity correction. Discrete think about decimals but continous are not. Think about how it is around and where to start to integrate. For instance to get P(X>=65), we might need to integrate from 64.5 if using continuous approximation.
    - A table of continuity correction:
    
          | Discrete probability question | Continuous probability question |
          |-------------------------------|---------------------------------|
          |              X=6              |            5.5<Y<6.5            |
          |              X>=6             |              Y>5.5              |
          |              X>6              |              y>6.5              |
          |              X<6              |              Y<5.5              |
          |              X<=6             |              Y<6.5              |
    
    - can approximate binomial when n large (>20), p is mid-ranged (np(1-p)>10)
    - recall poisson can also approximate binomial when n large (>20), p small (<0.05)
    - general idea: if there is a choice, go with the normal approximation
- Joint Distributions
  - RV interact with each other
  - notation P(A=1, B=1): probability that A takes value 1 **and** B takes value 1
  - For two discrete random variables X and Y, the joint probability mass function is: <a href="https://www.codecogs.com/eqnedit.php?latex=p_{X,Y}(a,b)&space;=&space;P(X=a,&space;Y=b)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?p_{X,Y}(a,b)&space;=&space;P(X=a,&space;Y=b)" title="p_{X,Y}(a,b) = P(X=a, Y=b)" /></a>
  - Marginal distributions: <a href="https://www.codecogs.com/eqnedit.php?latex=p_X&space;(a)&space;=&space;P(X=a)&space;=&space;\sum&space;_y&space;p_{X,Y}(a,y)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?p_X&space;(a)&space;=&space;P(X=a)&space;=&space;\sum&space;_y&space;p_{X,Y}(a,y)" title="p_X (a) = P(X=a) = \sum _y p_{X,Y}(a,y)" /></a>; <a href="https://www.codecogs.com/eqnedit.php?latex=p_Y&space;(b)&space;=&space;P(Y=b)&space;=&space;\sum&space;_x&space;p_{X,Y}(x,b)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?p_Y&space;(b)&space;=&space;P(Y=b)&space;=&space;\sum&space;_x&space;p_{X,Y}(x,b)" title="p_Y (b) = P(Y=b) = \sum _x p_{X,Y}(x,b)" /></a>
  - Multinomial distribution
    - n independent trials of experiment performed
    - each trial results in one of m outcomes with probabilit p1...pm
    - Xi = number of trials with outcome i
    - <a href="https://www.codecogs.com/eqnedit.php?latex=P(X_1&space;=&space;c_1,&space;X2&space;=&space;c_2,&space;...&space;X_m&space;=&space;c_m)&space;=&space;\binom{n}{c_1,&space;c_2,...,c_m}p_{1}^{c_1}...p_{m}^{c_m}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?P(X_1&space;=&space;c_1,&space;X2&space;=&space;c_2,&space;...&space;X_m&space;=&space;c_m)&space;=&space;\binom{n}{c_1,&space;c_2,...,c_m}p_{1}^{c_1}...p_{m}^{c_m}" title="P(X_1 = c_1, X2 = c_2, ... X_m = c_m) = \binom{n}{c_1, c_2,...,c_m}p_{1}^{c_1}...p_{m}^{c_m}" /></a>. LHS is a joint distribution, the first term in RHS represents multinomial number of ways of ordering the successes; the second term in RHS means probabilities of each ordering are equal and mutually exclusive. c1 + ... + cm = n and <a href="https://www.codecogs.com/eqnedit.php?latex=\binom{n}{c_1,&space;c_2,...,c_m}&space;=&space;\frac{n!}{c_1!...c_m!}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\binom{n}{c_1,&space;c_2,...,c_m}&space;=&space;\frac{n!}{c_1!...c_m!}" title="\binom{n}{c_1, c_2,...,c_m} = \frac{n!}{c_1!...c_m!}" /></a>
    - binomial: each trail has 2 possible outcomes; multinomial: each trial has m possible outcomes. It is a generalization of binomial
    - one interesting view in probabilistic text analysis: there are about 988,969 words in english. Can think of text as rolling a die with that amount of outcomes. So in this point of view, text is a multinomial.
- continuous joint distribution
  - A joint probability density function gives the relative likelihood of more than one continuous random variable each taking on a specific value
  - Marginal probabilities give the distribution of a subset of the variables (often just one) of a joint distribution
  - notation summary:
    - joint probability: <a href="https://www.codecogs.com/eqnedit.php?latex=P_{X,Y}(a,b)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?P_{X,Y}(a,b)" title="P_{X,Y}(a,b)" /></a> or P(X=a, Y=b)
    - continuous joint probability density: <a href="https://www.codecogs.com/eqnedit.php?latex=f_{X,Y}(a,b)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?f_{X,Y}(a,b)" title="f_{X,Y}(a,b)" /></a> or f(X=a, Y=b)
    - single (marginal) probability density function: <a href="https://www.codecogs.com/eqnedit.php?latex=f_{X}(a)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?f_{X}(a)" title="f_{X}(a)" /></a> or f(X=a)
    - single probability mass function: <a href="https://www.codecogs.com/eqnedit.php?latex=p_{X}(a)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?p_{X}(a)" title="p_{X}(a)" /></a> or P(X=a)
- An example that combines multinomial and Bayesain theorem
  - Say we know the articles written by A and B, we want to determine who writes article of C
  - from articles of A, we can get hi = probability that A writes word i (just count the number of times word i appears and divide the total number of words)
  - from articles of B, we can get mi = probability that B writes word i
  - we compare P(A|C) and P(B|C) which every is larger, then C is written by whom
  - <a href="https://www.codecogs.com/eqnedit.php?latex=P(A|C)&space;=&space;\frac{P(C|A)P(A)}{P(C)}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?P(A|C)&space;=&space;\frac{P(C|A)P(A)}{P(C)}" title="P(A|C) = \frac{P(C|A)P(A)}{P(C)}" /></a>, P(A) is our prior belief. Since we don't have prior knowledge about A and B, we set P(A) = P(B) = 1/2
    - ```P(C|A)``` can be think of as a multinomial distribution, A is like a bag of words, C is a distrubtion of some of its words. So ```P(C|A)``` = <a href="https://www.codecogs.com/eqnedit.php?latex=\binom{n}{n_1...n_m}\prod&space;h_1^{n_1}...h_m^{n_m}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\binom{n}{n_1...n_m}\prod&space;h_1^{n_1}...h_m^{n_m}" title="\binom{n}{n_1...n_m}\prod h_1^{n_1}...h_m^{n_m}" /></a> where ni is the number of times wordi appears in document C
    - P(C) is hard to manipulate but if we take the ratio of P(A|C) and P(B|C), it will be canceld
  - <a href="https://www.codecogs.com/eqnedit.php?latex=\frac{P(A|C)}{P(B|C)}&space;=&space;\frac{\prod&space;_i&space;h_i&space;^{n_i}}{\prod&space;_i&space;m_i^{n_i}}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\frac{P(A|C)}{P(B|C)}&space;=&space;\frac{\prod&space;_i&space;h_i&space;^{n_i}}{\prod&space;_i&space;m_i^{n_i}}" title="\frac{P(A|C)}{P(B|C)} = \frac{\prod _i h_i ^{n_i}}{\prod _i m_i^{n_i}}" /></a>
  - to avoid the number being too small, we can take the log and do: <a href="https://www.codecogs.com/eqnedit.php?latex=log(\frac{P(A|C)}{P(B|C)})&space;=&space;log(\frac{\prod&space;_i&space;h_i&space;^{n_i}}{\prod&space;_i&space;m_i^{n_i}})&space;=&space;\sum&space;_i&space;n_i&space;log(h_i)&space;-&space;\sum&space;_i&space;n_i&space;log(m_i)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?log(\frac{P(A|C)}{P(B|C)})&space;=&space;log(\frac{\prod&space;_i&space;h_i&space;^{n_i}}{\prod&space;_i&space;m_i^{n_i}})&space;=&space;\sum&space;_i&space;n_i&space;log(h_i)&space;-&space;\sum&space;_i&space;n_i&space;log(m_i)" title="log(\frac{P(A|C)}{P(B|C)}) = log(\frac{\prod _i h_i ^{n_i}}{\prod _i m_i^{n_i}}) = \sum _i n_i log(h_i) - \sum _i n_i log(m_i)" /></a>
  - the method here is similar to text classification in naive bayes
- Conditional joint distributions
  - jointly continuous function: <a href="https://www.codecogs.com/eqnedit.php?latex=P(a_1&space;<&space;X&space;<=&space;a_2,&space;b_1&space;<&space;Y&space;<=&space;b_2)&space;=&space;\int&space;_{a1}^{a_2}\int&space;_{b1}^{b_2}&space;f_{X,Y}(x,y)&space;dydx" target="_blank"><img src="https://latex.codecogs.com/gif.latex?P(a_1&space;<&space;X&space;<=&space;a_2,&space;b_1&space;<&space;Y&space;<=&space;b_2)&space;=&space;\int&space;_{a1}^{a_2}\int&space;_{b1}^{b_2}&space;f_{X,Y}(x,y)&space;dydx" title="P(a_1 < X <= a_2, b_1 < Y <= b_2) = \int _{a1}^{a_2}\int _{b1}^{b_2} f_{X,Y}(x,y) dydx" /></a>
  - cumulative density function in jointly continuous RV (jointly CDF): <a href="https://www.codecogs.com/eqnedit.php?latex=F_{X,Y}(a,b)=&space;\int&space;_{-\infty}^{a}\int&space;_{-\infty}^{b}&space;f_{X,Y}(x,y)&space;dydx" target="_blank"><img src="https://latex.codecogs.com/gif.latex?F_{X,Y}(a,b)=&space;\int&space;_{-\infty}^{a}\int&space;_{-\infty}^{b}&space;f_{X,Y}(x,y)&space;dydx" title="F_{X,Y}(a,b)= \int _{-\infty}^{a}\int _{-\infty}^{b} f_{X,Y}(x,y) dydx" /></a>
  - using joint CDF, we can avoid integral while computing the joint PDF: <a href="https://www.codecogs.com/eqnedit.php?latex=P(a_1&space;<&space;X&space;<=&space;a_2,&space;b_1&space;<&space;Y&space;<=&space;b_2)&space;=&space;F_{X,Y}(a_2,&space;b_2)&space;-&space;F_{X,Y}(a_1,&space;b_2)&space;-&space;F_{X,Y}(a_2,&space;b_1)&space;&plus;&space;F_{X,Y}(a_1,&space;b_1)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?P(a_1&space;<&space;X&space;<=&space;a_2,&space;b_1&space;<&space;Y&space;<=&space;b_2)&space;=&space;F_{X,Y}(a_2,&space;b_2)&space;-&space;F_{X,Y}(a_1,&space;b_2)&space;-&space;F_{X,Y}(a_2,&space;b_1)&space;&plus;&space;F_{X,Y}(a_1,&space;b_1)" title="P(a_1 < X <= a_2, b_1 < Y <= b_2) = F_{X,Y}(a_2, b_2) - F_{X,Y}(a_1, b_2) - F_{X,Y}(a_2, b_1) + F_{X,Y}(a_1, b_1)" /></a>
  - discrete conditional distributions
    - if X and Y are discrete random variables, conditional PMF of X given Y is: <a href="https://www.codecogs.com/eqnedit.php?latex=P_{X|Y}(x|y)&space;=&space;P(X=x&space;|&space;Y=y)&space;=&space;\frac{P(X=x,&space;Y=y)}{P(Y=y)}&space;=&space;\frac{p_{X,Y}(x,y)}{p_Y(y)}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?P_{X|Y}(x|y)&space;=&space;P(X=x&space;|&space;Y=y)&space;=&space;\frac{P(X=x,&space;Y=y)}{P(Y=y)}&space;=&space;\frac{p_{X,Y}(x,y)}{p_Y(y)}" title="P_{X|Y}(x|y) = P(X=x | Y=y) = \frac{P(X=x, Y=y)}{P(Y=y)} = \frac{p_{X,Y}(x,y)}{p_Y(y)}" /></a>
  - continuous conditional distributions
    - let X and Y be continuous random variables, use PDF to compute conditional joint probability. The idea is that we can turn PDF to probability by multiply a small distance epsilon and it will cancel out. So <a href="https://www.codecogs.com/eqnedit.php?latex=f_{X|Y}(x|y)&space;=&space;\frac{f_{X,Y}(x,y)}{f_Y(y)}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?f_{X|Y}(x|y)&space;=&space;\frac{f_{X,Y}(x,y)}{f_Y(y)}" title="f_{X|Y}(x|y) = \frac{f_{X,Y}(x,y)}{f_Y(y)}" /></a>
  - Bayes revisited: <a href="https://www.codecogs.com/eqnedit.php?latex=P(B|E)&space;=&space;\frac{P(E|B)P(B)}{P(E)}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?P(B|E)&space;=&space;\frac{P(E|B)P(B)}{P(E)}" title="P(B|E) = \frac{P(E|B)P(B)}{P(E)}" /></a>
    - ```P(B|E)``` is called posterior belief; ```P(E|B)``` is the likelihood of evidence; P(B) is prior belief; P(E) is normalization constant.
  - Mixing discrete and continuous:
    - Let X be a continuous random variable, let N be a discrete random variable, consider <a href="https://www.codecogs.com/eqnedit.php?latex=P(X=x|N=n)&space;=&space;\frac{P(N=n|X=x)P(X=x)}{P(N=n)}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?P(X=x|N=n)&space;=&space;\frac{P(N=n|X=x)P(X=x)}{P(N=n)}" title="P(X=x|N=n) = \frac{P(N=n|X=x)P(X=x)}{P(N=n)}" /></a>
    -  We have <a href="https://www.codecogs.com/eqnedit.php?latex=f_{X|N}(x|n)\epsilon&space;_x&space;=&space;\frac{P_{N|X}(n|x)f_X(x)\epsilon&space;_x}{P_N(n)}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?f_{X|N}(x|n)\epsilon&space;_x&space;=&space;\frac{P_{N|X}(n|x)f_X(x)\epsilon&space;_x}{P_N(n)}" title="f_{X|N}(x|n)\epsilon _x = \frac{P_{N|X}(n|x)f_X(x)\epsilon _x}{P_N(n)}" /></a>
    - Again the idea is the same, for continuous RV, we use PDF times epsilon; for discrete RV, we use PMF directly
    - Similarly, the LHS can also be PMF and RHS has PDF. Or it can all be continuous, all have PDF
  - Bivariate normal:
    - X,Y follow a symmetric bivariate normal distribution if they have joint PDF:
      - <a href="https://www.codecogs.com/eqnedit.php?latex=f_{X,Y}(x,y)&space;=&space;\frac{1}{2&space;\pi&space;\sigma^2}e^{-\frac{[(x-\mu_x)^2&space;&plus;&space;(y-\mu_y)^2]}{2&space;\sigma^2}}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?f_{X,Y}(x,y)&space;=&space;\frac{1}{2&space;\pi&space;\sigma^2}e^{-\frac{[(x-\mu_x)^2&space;&plus;&space;(y-\mu_y)^2]}{2&space;\sigma^2}}" title="f_{X,Y}(x,y) = \frac{1}{2 \pi \sigma^2}e^{-\frac{[(x-\mu_x)^2 + (y-\mu_y)^2]}{2 \sigma^2}}" /></a>
      - it is a 2D Gaussian
  - continuous conditional joint probability
    - goal f(X=x, Y=y | D=d)
    - X and Y are Gaussian, we have f(X=x, Y=y) which is prior
    - we have f(D=d | X=x, Y=y), D is observation
    - <a href="https://www.codecogs.com/eqnedit.php?latex=f(X=x,&space;Y=y&space;|&space;D=d)&space;=&space;\frac{f(D=d|X=x,&space;Y=y)f(X=x,&space;Y=y)}{f(D=d)}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?f(X=x,&space;Y=y&space;|&space;D=d)&space;=&space;\frac{f(D=d|X=x,&space;Y=y)f(X=x,&space;Y=y)}{f(D=d)}" title="f(X=x, Y=y | D=d) = \frac{f(D=d|X=x, Y=y)f(X=x, Y=y)}{f(D=d)}" /></a> where f(D=d) is just a constant for a given d because it is not based on x and y
  - expectation of multiple RVs
    - expectation over a joint isn't nicely defined because it is not clear how to compose the multiple variables
    - lemma: for a function g(X,Y) we can calculate the expectation of that function:
      - E[g(X,Y)] = <a href="https://www.codecogs.com/eqnedit.php?latex=\sum&space;_{x,y}&space;g(x,y)p(x,y)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\sum&space;_{x,y}&space;g(x,y)p(x,y)" title="\sum _{x,y} g(x,y)p(x,y)" /></a>
      - recall for a single RV: E[g(X)] = <a href="https://www.codecogs.com/eqnedit.php?latex=\sum&space;_{x}&space;g(x)p(x)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\sum&space;_{x}&space;g(x)p(x)" title="\sum _{x} g(x)p(x)" /></a>
  - independence of RVs
    - two discrete random variables X and Y are **independent** if: P(X=x, Y=y) = P(X=x)P(Y=y)
    - to check independent of RVs, the above equation need to hold for all combinations of RVs
      - but for an event, it just need to hold for one combination
    - intuitively: knowing the value of X tells us nothing about the distribution of Y (and vice versa)
    - for continuous variables
      - two continuous RVs X and Y are **independent** if:
        - P(X<=a, Y<=b) = P(X<=a)P(Y<=b) for any a,b
        - or for any a,b <a href="https://www.codecogs.com/eqnedit.php?latex=f_{X,Y}(a,b)&space;=&space;f_X(a)f_Y(b)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?f_{X,Y}(a,b)&space;=&space;f_X(a)f_Y(b)" title="f_{X,Y}(a,b) = f_X(a)f_Y(b)" /></a>
        - or factorize CDF: <a href="https://www.codecogs.com/eqnedit.php?latex=F_{X,Y}(a,b)&space;=&space;F_X(a)F_Y(b)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?F_{X,Y}(a,b)&space;=&space;F_X(a)F_Y(b)" title="F_{X,Y}(a,b) = F_X(a)F_Y(b)" /></a>
        - more generally joint density factors separately <a href="https://www.codecogs.com/eqnedit.php?latex=f_{X,Y}(a,b)&space;=&space;h(x)g(y)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?f_{X,Y}(a,b)&space;=&space;h(x)g(y)" title="f_{X,Y}(a,b) = h(x)g(y)" /></a> where x>-\infinity y<\infinity
